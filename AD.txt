AD
thread_iWZpBtOMexEEbMYAqB4Uo8Co
User
Explore built-in plugins Completed100 XP 5 minutes Plugins are one of the core components of the Semantic Kernel SDK. A plugin defines the task for the kernel to perform as it interfaces with the large language model (LLM). Plugins can be composed of native code and prompts to the LLM. The Semantic Kernel SDK also offers some built-in plugins for common tasks that are ready for use in your applications. What is a plugin? In the Semantic Kernel SDK, a plugin is a class that contains functions that can be run by the kernel. A plugin function can be made from a semantic prompt or native function code. To use a plugin, you add it to the kernel and then call the desired function using the InvokeAsync method. The kernel accesses the plugin, locates and runs the function, then returns the result. Let's take a look at some predefined plugins. Built-in plugins The Semantic Kernel SDK offers an extra package with predefined plugins for common tasks. These are available in the Plugins.Core package that you can install with NuGet: dotnet add package Microsoft.SemanticKernel.Plugins.Core --version 1.2.0-alpha The package includes the following plugins: ConversationSummaryPlugin - Summarizes conversation FileIOPlugin - Reads and writes to the filesystem HttpPlugin - Makes requests to HTTP endpoints MathPlugin - Performs mathematical operations TextPlugin - Performs text manipulation TimePlugin - Gets time and date information WaitPlugin - Pauses execution for a specified amount of time To use a core plugin, you can add it to your kernel builder using the AddFromType method. For example, to add the TimePlugin to your kernel, you can use the following code: c#Copy using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Plugins.Core; var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( "your-deployment-name", "your-endpoint", "your-api-key", "deployment-model"); builder.Plugins.AddFromType<TimePlugin>(); var kernel = builder.Build(); var currentDay = await kernel.InvokeAsync("TimePlugin", "DayOfWeek"); Console.WriteLine(currentDay); This code adds the TimePlugin to the kernel and then calls the DayOfWeek function to get the current day of the week. The output is the current day of the week. These readily available plugins can be used to quickly add functionality to your project. In the next exercise, you'll learn how the ConversationSummaryPlugin can be used to get a summary of a conversation, or the action items from a block of text. Next unit: Exercise - Use built-in plugins Continue

User
Optimize language model prompts Completed100 XP 8 minutes Prompts are conversational cues you give to large language models (LLMs), shaping responses based on your queries or instructions. For example, you can prompt LLMs to convert a sentence from English to French, or to generate a summary of a text. In the previous unit, you created the prompt as the input string: c#Copy string input = @"I'm a vegan in search of new recipes. I love spicy food! Can you give me a list of breakfast recipes that are vegan friendly?"; Prompting involves crafting clear, context rich instructions to guide the model to generate a desired response. To craft an effective prompt, precision and clarity are key. You may need to experiment and adjust your prompts for accurate results. Tips for crafting prompts Specific Inputs Yield Specific Outputs: LLMs respond based on the input they receive. Crafting clear and specific prompts is crucial to get the desired output. Experimentation is Key: You may need to iterate and experiment with different prompts to understand how the model interprets and generates responses. Small tweaks can lead to significant changes in outcomes. Context Matters: LLMs consider the context provided in the prompt. You should ensure that the context is well-defined and relevant to obtain accurate and coherent responses. Handle Ambiguity: Bear in mind that LLMs may struggle with ambiguous queries. Provide context or structure to avoid vague or unexpected results. Length of Prompts: While LLMs can process both short and long prompts, you should consider the trade-off between brevity and clarity. Experimenting with prompt length can help you find the optimal balance. Create prompt templates The Semantic Kernel SDK supports a templating language that allows you to use expressions and variables in your natural language prompts. This means you can create prompts that are reusable with different input parameters. To embed expressions in your prompts, the templating language uses curly brackets {{...}}. c#Copy using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Plugins.Core; var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( "your-deployment-name", "your-endpoint", "your-api-key", "deployment-model"); builder.Plugins.AddFromType<ConversationSummaryPlugin>(); var kernel = builder.Build(); string history = @"In the heart of my bustling kitchen, I have embraced the challenge of satisfying my family's diverse taste buds and navigating their unique tastes. With a mix of picky eaters and allergies, my culinary journey revolves around exploring a plethora of vegetarian recipes. One of my kids is a picky eater with an aversion to anything green, while another has a peanut allergy that adds an extra layer of complexity to meal planning. Armed with creativity and a passion for wholesome cooking, I've embarked on a flavorful adventure, discovering plant-based dishes that not only please the picky palates but are also heathy and delicious."; string prompt = @"This is some information about the user's background: {{$history}} Given this user's background, provide a list of relevant recipes."; var result = await kernel.InvokePromptAsync(prompt, new KernelArguments() {{ "history", history }}); Console.WriteLine(result); In this example, the history variable is referenced in the prompt, denoted by the $ symbol. When the prompt is invoked, the history variable is replaced with the actual value provided in the KernelArguments dictionary. This allows you to create prompts that can be dynamically populated with different inputs. Here's the example output: OutputCopy

Lentil and vegetable soup - a hearty, filling soup that is perfect for a cold day. This recipe is vegetarian and can easily be adapted to accommodate allergies. 2. Cauliflower "steaks" - a delicious and healthy main course that is sure to satisfy even the pickiest of eaters. This recipe is vegetarian and can easily be made vegan. 3. Quinoa salad with roasted vegetables - a healthy and filling salad that is perfect for any occasion. This recipe is vegetarian and can easily be adapted to accommodate allergies. 4. Peanut-free pad Thai - a classic dish made without peanut sauce, perfect for those with peanut allergies. This recipe is vegetarian and can easily be made vegan. 5. Black bean and sweet potato enchiladas - a delicious and healthy twist on traditional enchiladas. This recipe is vegetarian and can easily be made vegan. Creating reusable prompts is especially useful when you need to perform the same task with different inputs. In the next exercise, you'll practice creating your own reusable prompts using the Semantic Kernel SDK. Next unit: Exercise - Write your own prompt Continue
User
Exercise - Use built-in plugins Completed 100 XP 10 minutes For this exercise, you add an existing plugin to your Semantic Kernel project and learn how to use your first plugin to accomplish a task. Let's get started! Prepare your development environment For these exercises, a starter project is available for you to use. Use the following steps to set up the starter project: Important You must have Visual Studio Code and the .NET Framework 8.0 installed to complete these steps. You may also need to install the Visual Studio Code C# Dev Kit extension. Open Visual Studio Code. Under the Visual Studio Code Start section, select Clone Git Repository. In the URL bar, enter https://github.com/MicrosoftLearning/MSLearn-Develop-AI-Agents-with-Azure-OpenAI-and-Semantic-Kernel-SDK.git In the File Explorer, create a new folder in a location that is easy to find and remember, such as a folder in your Desktop. Click the Select as Repository Destination button. You need to be signed in to GitHub to successfully clone the project. Open the project in Visual Studio Code. In Explorer, right-click the M02-create-plugins-for-semantic-kernel/M02-Project folder and click Open in Integrated Terminal. Expand the M02-create-plugins-for-semantic-kernel/M02-Project folder. You should see a "Program.cs" file. Open the Program.cs file and update the following variables with your Azure OpenAI Services deployment name, API key, endpoint. C# Copy string yourDeploymentName = ""; string yourEndpoint = ""; string yourKey = ""; Now you're ready to begin the exercise. Good luck! Get the user's intent In the Integrated Terminal, add the Semantic Kernel Plugins. Core package to your project by running the following command: dotnet add package Microsoft.SemanticKernel.Plugins.Core --version 1.2.0-alpha Note Currently the package is a prerelease version. You may want to check for the latest version https://www.nuget.org/packages/Microsoft.SemanticKernel.Plugins.Core/1.2.0-alpha Add the following code to your Program.cs file: c# Copy builder.Plugins.AddFromType<ConversationSummaryPlugin>(); var kernel = builder.Build(); string input = @"I'm a vegan in search of new recipes. I love spicy food! Can you give me a list of breakfast recipes that are vegan friendly?"; var result = await kernel.InvokeAsync( "ConversationSummaryPlugin", "GetConversationActionItems", new() {{ "input", input }}); Console.WriteLine(result); In this code, you import the ConversationSummaryPlugin from the Core.Plugins package. You create a new kernel builder and add the plugin to the builder. The ConversationSummaryPlugin supports several actions: GetConversationActionItems, SummarizeConversation, and GetConversationTopics. You create a string with a sample conversation, then call the plugin with the function name and pass in the conversation string. The plugin returns a summary of the conversation. Note Since the plugin is still in preview, you may need to add suppress the warning by using #pragma warning disable SKEXP0050 at the top of the file. Run the code by entering dotnet run in the terminal. You should see a response similar to the following output: Output Copy { "actionItems": [ { "owner": "", "actionItem": "Provide a list of vegan-friendly spicy breakfast recipes", "dueDate": "", "status": "Open", "notes": "" } ] } Notice that the plugin was able to identify a simplified intent from the input string. This example is just the beginning of what you can do with the Semantic Kernel SDK. In the next unit, you'll learn more ways to customize plugins and run functions. Important Be sure not to delete any of the code you wrote, you need it for the next exercises. given this lesson , this is my current Program.cs rerwriteit for me ChatGPT said: ChatGPT Based on the instructions and the current requirements of your exercise, we'll rewrite the Program.cs to properly set up the Semantic Kernel environment, add the ConversationSummaryPlugin, and invoke it as described in the lesson. Rewritten Program.cs csharp Copy code using System; using System.Threading.Tasks; using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Plugins.Core; // Import the namespace for core plugins #pragma warning disable SKEXP0050 // Suppress warning for using preview plugin class Program { static async Task Main(string[] args) { // Azure OpenAI deployment details string yourDeploymentName = "geaux-openai-deployment"; // Replace with your actual deployment name string yourEndpoint = "https://geaux-openai.openai.azure.com/"; // Replace with your actual endpoint string yourApiKey = "03cfbaafba5c421392e8473223bc73d9"; // Replace with your actual API key // Create a kernel builder and configure it with Azure OpenAI Chat Completion var builder = Kernel.Builder; builder.AddAzureOpenAIChatCompletion( deploymentName: yourDeploymentName, endpoint: yourEndpoint, apiKey: yourApiKey, deploymentModel: "gpt-35-turbo-16k" ); // Register the ConversationSummaryPlugin with the kernel builder.Plugins.AddFromType<ConversationSummaryPlugin>(); // Build the kernel var kernel = builder.Build(); // Define the input prompt string input = @"I'm a vegan in search of new recipes. I love spicy food! Can you give me a list of breakfast recipes that are vegan friendly?"; // Invoke the plugin function asynchronously var result = await kernel.InvokeAsync( pluginName: "ConversationSummaryPlugin", functionName: "GetConversationActionItems", inputParameters: new() { { "input", input } } ); // Output the result Console.WriteLine(result); } } Key Steps and Adjustments: Setting Azure OpenAI Deployment Details: Replace yourDeploymentName, yourEndpoint, and yourApiKey with your actual Azure OpenAI Service details. Plugin Registration: Used builder.Plugins.AddFromType<ConversationSummaryPlugin>(); to add the plugin to the kernel. Building the Kernel: Constructed the kernel using builder.Build(). Invoke the Function: Used kernel.InvokeAsync with the plugin name, function name, and input parameters to call the plugin method. How to Run: Save the changes to your Program.cs file. Run the project using the following command in the terminal: bash Copy code dotnet run This updated code should align perfectly with the instructions provided in your exercise. If you have any further issues or additional instructions, feel free to ask! 4o You said: this is my current Program.cs file #pragma warning disable SKEXP0050 using Microsoft.SemanticKernel; string yourDeploymentName = "geaux-openai-deployment"; string yourEndpoint = "https://geaux-openai.openai.azure.com/openai/deployments/geaux-openai-deployment/chat/completions?api-version=2023-03-15-preview"; string yourApiKey = "03cfbaafba5c421392e8473223bc73d9"; var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( yourDeploymentName, yourEndpoint, yourApiKey, "gpt-35-turbo-16k"); var kernel = builder.Build(); var result = await kernel.InvokePromptAsync( "Give me a list of breakfast foods with eggs and cheese"); Console.WriteLine(result); can you fix it ChatGPT said: ChatGPT Let's fix your Program.cs file to align with the Semantic Kernel SDK's expected usage pattern for invoking a plugin function. Key Issues to Address: Correct API Endpoint Format: The yourEndpoint variable should point to the endpoint URL without including specific paths for deployment and completion. It should be the base endpoint. Plugin Function Invocation: The InvokePromptAsync method is being used to run a prompt directly, which might not align with the plugin's functionality that you're supposed to use. We need to ensure that you are using the InvokeAsync method correctly for a function from a plugin. Context Creation for Inputs: We should properly handle input parameters using the SDK's methods. Corrected Program.cs Here’s the revised version of your code: csharp Copy code #pragma warning disable SKEXP0050 using System; using System.Threading.Tasks; using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Plugins.Core; // Import necessary plugins class Program { static async Task Main(string[] args) { // Azure OpenAI deployment details string yourDeploymentName = "geaux-openai-deployment"; string yourEndpoint = "https://geaux-openai.openai.azure.com/"; // Use the base endpoint URL string yourApiKey = "03cfbaafba5c421392e8473223bc73d9"; // Create a kernel builder and configure it with Azure OpenAI Chat Completion var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( deploymentName: yourDeploymentName, endpoint: yourEndpoint, apiKey: yourApiKey, deploymentModel: "gpt-35-turbo-16k" ); // Build the kernel var kernel = builder.Build(); // Define the input prompt string input = @"Give me a list of breakfast foods with eggs and cheese"; // Invoke the function asynchronously var result = await kernel.InvokePromptAsync(input); // Output the result Console.WriteLine(result); } } Key Changes: Fixed Endpoint: The yourEndpoint variable now only includes the base URL for your Azure OpenAI instance (https://geaux-openai.openai.azure.com/). The specific API path and versioning are managed by the SDK internally. Simplified Function Call: The method InvokePromptAsync is retained if you want to run a simple prompt directly. If you were supposed to use a specific plugin function, you might need InvokeAsync with different parameters. How to Run: Save the updated Program.cs file. Run the project using the following command in the terminal: bash Copy code dotnet run This should now align with your intended usage and avoid any errors related to incorrect endpoints or method usage. If there's a need to invoke a specific function from a plugin rather than running a prompt, please clarify, and I can adjust the example accordingly. 4o You said: [{ "resource": "/home/codespace/MSLearn-Develop-AI-Agents-with-Azure-OpenAI-and-Semantic-Kernel-SDK-1/M02-create-plugins-for-semantic-kernel/M02-Project/Program.cs", "owner": "DocumentCompilerSemantic", "code": { "value": "CS1739", "target": { "$mid": 1, "path": "/query/roslyn.query", "scheme": "https", "authority": "msdn.microsoft.com", "query": "appId=roslyn&k=k(CS1739)" } }, "severity": 8, "message": "The best overload for 'AddAzureOpenAIChatCompletion' does not have a parameter named 'deploymentModel'", "startLineNumber": 22, "startColumn": 13, "endLineNumber": 22, "endColumn": 28 }] ChatGPT said: ChatGPT The error you're encountering indicates that the AddAzureOpenAIChatCompletion method doesn't have a parameter named deploymentModel. This means we need to adjust the method call to match the correct parameters that the SDK expects. Let's correct the Program.cs file by removing the incorrect deploymentModel parameter and ensuring the method signature matches what the SDK provides. Corrected Program.cs Here’s the updated code: csharp Copy code #pragma warning disable SKEXP0050 using System; using System.Threading.Tasks; using Microsoft.SemanticKernel; class Program { static async Task Main(string[] args) { // Azure OpenAI deployment details string yourDeploymentName = "geaux-openai-deployment"; string yourEndpoint = "https://geaux-openai.openai.azure.com/"; string yourApiKey = "03cfbaafba5c421392e8473223bc73d9"; // Create a kernel builder and configure it with Azure OpenAI Chat Completion var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( deploymentName: yourDeploymentName, endpoint: yourEndpoint, apiKey: yourApiKey ); // Build the kernel var kernel = builder.Build(); // Define the input prompt string input = @"Give me a list of breakfast foods with eggs and cheese"; // Invoke the prompt asynchronously var result = await kernel.InvokePromptAsync(input); // Output the result Console.WriteLine(result); } } Key Adjustments: Removed deploymentModel Parameter: The deploymentModel parameter was removed because the method AddAzureOpenAIChatCompletion doesn't support it according to the error. Ensure Correct Parameter Names: The parameters deploymentName, endpoint, and apiKey are correctly passed to the method. How to Run: Save the updated Program.cs file. Run the project using the following command in the terminal: bash Copy code dotnet run This should resolve the issue related to the incorrect parameter name and align the method call with the expected SDK usage. If further errors arise, they might be related to other parts of the configuration or SDK, which we can troubleshoot together.

User
Exercise - Use personas in prompts Completed100 XP 15 minutes Assigning personas to your prompts can improve the quality of the responses generated by the large language model (LLM). Personas provide context to the LLM, allowing it to generate responses that are better aligned with your user's intent. Let's try it out! Open the Visual Studio Code project you created in the previous exercise. Update your prompt from the previous exercise with the following text: C#Copy using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Plugins.Core; var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( "your-deployment-name", "your-endpoint", "your-api-key", "deployment-model"); var kernel = builder.Build(); string language = "French"; string history = @"I'm traveling with my kids and one of them has a peanut allergy."; // Assign a persona to the prompt string prompt = @$" You are a travel assistant. You are helpful, creative, and very friendly. Consider the traveler's background: ${history} Create a list of helpful phrases and words in ${language} a traveler would find useful. Group phrases by category. Include common direction words. Display the phrases in the following format: Hello - Ciao [chow] Begin with: 'Here are some phrases in ${language} you may find helpful:' and end with: 'I hope this helps you on your trip!'"; var result = await kernel.InvokePromptAsync(prompt); Console.WriteLine(result); Run the code by entering dotnet run in the terminal. If you run the code, you might notice the responses are more consistent than your previous results. The LLM is more likely to generate a response that matches the persona you assigned and the context of the task. Your response may look similar to the following output: OutputCopy Here are some phrases in French you may find helpful: Greetings: - Hello - Bonjour [bon-zhur] - Goodbye - Au revoir [oh ruh-vwar] - Thank you - Merci [mehr-see] Directions: - Go straight ahead - Allez tout droit [ah-lay too dwa] - Turn left/right - Tournez à gauche/droite [toor-nay ah gohsh/dwaht] - It's on the left/right - C'est à gauche/droite [say ah gohsh/dwaht] Food: - Does this contain peanuts? - Est-ce que cela contient des cacahuètes? [ess-kuh suh suh-la kohn-tee-eh day kah-kah-weht?] - My child has a peanut allergy - Mon enfant est allergique aux cacahuètes [mohn ahn-fahn ay ah-lair-gee-k oh kah-kah-weht] ... I hope this helps you on your trip! You can also provide instructions to the LLM to assume a role when generating a response, and provide example requests and responses. In Semantic Kernel, a special syntax is used to define message roles. To define a message role, you can wrap the message in <message> tag with the role name as an attribute. The supported roles are 'user', 'system', 'assistant', and 'bot'. Let's try it out! Update your prompt with the following text: C#Copy string prompt = @$" The following is a conversation with an AI travel assistant. The assistant is helpful, creative, and very friendly. <message role=""user"">Can you give me some travel destination suggestions?</message> <message role=""assistant"">Of course! Do you have a budget or any specific activities in mind?</message> <message role=""user"">${input}</message>"; Next, let's update the input to provide the AI with some details for the trip. Update the input string to the following text: c#Copy string input = @"I'm planning an anniversary trip with my spouse. We like hiking, mountains, and beaches. Our travel budget is $15000"; Next, run the code and observe how the LLM responds. Enter dotnet run in the terminal. OutputCopy That sounds like a great trip ahead! Here are a few suggestions: 1. New Zealand - With stunning mountain ranges, iconic hiking trails, and beautiful beaches, New Zealand is a popular destination for outdoor enthusiasts. Some must-visit spots include the Milford Track, Fox Glacier, and Abel Tasman National Park. 2. Hawaii - Known for its picturesque beaches, Hawaii is also home to several stunning hiking trails. The Kalalau Trail on Kauai is a popular trail that offers breathtaking views of the Na Pali Coast. 3. Costa Rica - Costa Rica boasts beautiful beaches and breathtaking mountains. Hike through the Monteverde Cloud Forest Reserve and catch a glimpse of exotic wildlife like the resplendent quetzal, or take a dip in the turquoise waters of Playa Manuel Antonio. 4. Banff National Park, Canada - Located in the Canadian Rockies, Banff National Park offers some of the most stunning mountain scenery in the world. Explore the park's many hiking trails, relax in hot springs, and take in the beauty of the Canadian wilderness. 5. Amalfi Coast, Italy - The Amalfi Coast is a picturesque stretch of coastline in Southern Italy that offers stunning views of the Mediterranean Sea. Take a hike along the famous Path of the Gods or enjoy a romantic stroll through one of the Amalfi Coast's charming towns like Positano or Ravello. These are just a few of many options, but with a budget of $15000, you should be able to have a fantastic trip to any of these destinations! Notice how assigning a persona to the LLM allows you to create a more natural and personalized conversation. You can also tune prompts to be less verbose and only output specific information. For example, suppose the user wants to get a list of flights from one destination to another. You can ask the LLM to parse their input and return only the relevant information in a format you can use in your code. Let's try it out! Update your prompt to the following text: c#Copy string prompt = @$" <message role=""system"">Instructions: Identify the from and to destinations and dates from the user's request</message> <message role=""user"">Can you give me a list of flights from Seattle to Tokyo? I want to travel from March 11 to March 18.</message> <message role=""assistant"">Seattle|Tokyo|03/11/2024|03/18/2024</message> <message role=""user"">${input}</message>"; In this prompt, we use the <message> and also provide an example for the LLM. We want to format the output in a way that we can parse, so we provide that format in the example. Next, let's update the input to provide the AI with some details for the trip. Modify the input to the following text: c#Copy string input = @"I have a vacation from June 1 to July 22. I want to go to Greece. I live in Chicago."; Run the code by entering dotnet run in the terminal. OutputCopy Chicago|Greece|06/01/2024|07/22/2024 Notice how the LLM was able to parse the input and return only the relevant information. Prompting the LLM to parse data is a great way to quickly get the information you need from the user. Important Be sure not to delete any of the code you wrote so far, you need it for the next exercise.

User
Skip to main content Learn Discover Product documentation Development languages Topics

Training Products Career Paths Browse all training Educator Center Student Hub FAQ & Help LEVEL 32500 /3699 XP Learn Training Browse Develop AI agents using Azure OpenAI and the Semantic Kernel SDK Create plugins for semantic kernel Save Previous Unit 8 of 10 Next Exercise - Saving prompts to files Completed100 XP 15 minutes Suppose you want to suggest travel destinations and recommend activities for a user. In this exercise, you practice creating prompts and saving them to files. Let's get started! Open the Visual Studio Code project you created in the previous exercise. In the Program.cs file, remove the prompt and input variables you created in the previous exercise so that you are left with the following code: C#Copy using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Plugins.Core; var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( "your-deployment-name", "your-endpoint", "your-api-key", "deployment-model"); var kernel = builder.Build(); Verify the following folders exist in your project: 'Prompts' 'Prompts/TravelPlugins' 'Prompts/TravelPlugins/SuggestDestinations' 'Prompts/TravelPlugins/GetDestination' 'Prompts/TravelPlugins/SuggestActivities' These directories will help organize your prompts. First you create a prompt that identifies the destination a user wants to travel to. To create the prompt, you need to create the config.json and the skprompt.txt files. Let's get started! In the GetDestination folder, open the config.json file and enter the following code: JSONCopy { "schema": 1, "type": "completion", "description": "Identify the destination of the user's travel plans", "execution_settings": { "default": { "max_tokens": 1200, "temperature": 0 } }, "input_variables": [ { "name": "input", "description": "Text from the user that contains their travel destination", "required": true } ] } This configuration tells the kernel what your prompt does and what input variables to accept. Next, you provide the prompt text in the skprompt.txt file. In GetDestination folder, open the skprompt.txt file and enter the following text: HTMLCopy <message role="system"> Instructions: Identify the destination the user wants to travel to. </message> <message role="user"> I am so excited to take time off work! My partner and I are thinking about going to Santorini in Greece! I absolutely LOVE Greek food, I can't wait to be some place warm. </message> <message role="assistant">Santorini, Greece</message> <message role="user">{{$input}}</message> This prompt helps the large language model (LLM) filter the user's input and retrieve just the destination from the text. In the SuggestDestinations folder, open the config.json file and enter the following text: JSONCopy { "schema": 1, "type": "completion", "description": "Recommend travel destinations to the user", "execution_settings": { "default": { "max_tokens": 1200, "temperature": 0.3 } }, "input_variables": [ { "name": "input", "description": "Details about the user's travel plans", "required": true } ] } In this config, you can raise the temperature a bit to make the output more creative. In the SuggestDestinations folder, open the skprompt.txt file and enter the following text: HTMLCopy The following is a conversation with an AI travel assistant. The assistant is helpful, creative, and very friendly. <message role="user">Can you give me some travel destination suggestions?</message> <message role="assistant">Of course! Do you have a budget or any specific activities in mind?</message> <message role="user">${input}</message> This prompt suggests travel destinations to the user based on their input. Now let's create a plugin to recommend activities at their destination. In the SuggestActivities folder, open the config.json file and enter the following text: JSONCopy { "schema": 1, "type": "completion", "description": "Recommend activities at a travel destination to the user", "execution_settings": { "default": { "max_tokens": 4000, "temperature": 0.3 } }, "input_variables": [ { "name": "history", "description": "Background information about the user", "required": true }, { "name": "destination", "description": "The user's travel destination", "required": true } ] } In this config, you increase the max_tokens to allow more text for the history and generated text. In the SuggestActivities folder, open the skprompt.txt file and enter the following text: HTMLCopy You are a travel assistant. You are helpful, creative, and very friendly. Consider your previous conversation with the traveler: {{$history}} The traveler would like some activity recommendations, things to do, and points of interest for their trip. They want to go to {{$destination}}. Please provide them with a list of things they might like to do at their chosen destination. Now let's import and test your new prompts! Update your Program.cs file with the following code: c#Copy using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Plugins.Core; using Microsoft.SemanticKernel.ChatCompletion; var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( "your-deployment-name", "your-endpoint", "your-api-key", "deployment-model"); var kernel = builder.Build(); kernel.ImportPluginFromType<ConversationSummaryPlugin>(); var prompts = kernel.ImportPluginFromPromptDirectory("Prompts/TravelPlugins"); ChatHistory history = []; string input = @"I'm planning an anniversary trip with my spouse. We like hiking, mountains, and beaches. Our travel budget is $15000"; var result = await kernel.InvokeAsync<string>(prompts["SuggestDestinations"], new() {{ "input", input }}); Console.WriteLine(result); history.AddUserMessage(input); history.AddAssistantMessage(result); In this code, you import the plugins you created. You also use a ChatHistory object to store the user's conversation. Finally, you pass some information to the SuggestDestinations prompt and record the results. Next, let's ask the user where they want to go so we can recommend some activities to them. Add the following code to your Program.cs file: c#Copy Console.WriteLine("Where would you like to go?"); input = Console.ReadLine(); result = await kernel.InvokeAsync<string>(prompts["SuggestActivities"], new() { { "history", history }, { "destination", input }, } ); Console.WriteLine(result); In this code, you get some input from the user to find out where they want to go. Then you call the SuggestActivities prompt with the destination and the conversation history. To test the code, enter dotnet run in the terminal. The final output may look similar to the following: OutputCopy Absolutely! Japan is a wonderful destination with so much to see and do. Here are some recommendations for activities and points of interest: 1. Visit Tokyo Tower: This iconic tower offers stunning views of the city and is a must-visit attraction. 2. Explore the temples of Kyoto: Kyoto is home to many beautiful temples, including the famous Kiyomizu-dera and Fushimi Inari-taisha. 3. Experience traditional Japanese culture: Attend a tea ceremony, try on a kimono, or take a calligraphy class to immerse yourself in Japanese culture. Now you created the beginnings of an AI travel assistant! Try changing the input to see how the LLM responds. Next unit: Knowledge check Continue Need help? See our troubleshooting guide or provide specific feedback by reporting an issue. Feedback Was this page helpful? YesNo English (United States) Your Privacy Choices Theme Previous Versions Blog Contribute Privacy Terms of Use Trademarks © Microsoft 2024

User
Introduction Completed100 XP 3 minutes One of the core features of the Semantic Kernel SDK is that it allows developers to combine native code with the power of large language models (LLMs). Plugins containing native functions can be used to extend the functionality of your artificially intelligent (AI) agent. These native functions can perform a wide range of tasks, from complex calculations to personalized recommendations. Suppose you want to create a smart music service that can provide a personalized listening experience. You can use native functions to interact with a music library, add songs to a user's "Recently Played" list, and then use a semantic prompt to recommend songs and concerts. In this module, you learn how native functions work in the Semantic Kernel SDK. You can understand how native functions accomplish customized tasks, effectively giving your AI agent a "skill." Learning objectives Understand native functions in the Semantic Kernel SDK. Learn how to create native function plugins. Prerequisites Experience programming in C#. Visual Studio Code IDE installed. Familiarity with Azure and the Azure portal. Access to Azure OpenAI Services. Experience using the Semantic Kernel SDK to create prompts.

User
Understand native functions Completed100 XP 10 minutes Native functions are functions written in native code that can be called by the Semantic Kernel SDK in your application. They're useful for performing tasks that large language models (LLM) can't do on their own. You can think of a native function like a skill your application can perform. In later modules, you'll learn how to use the Semantic Kernel to automatically invoke native functions you create, and combine your logic with prompts to the LLM. Combining chat completion services with functional skills allows you to create an AI agent that can perform a wide range of tasks. But for now, let's focus on designing native functions. Native functions have a certain format and a recommended file structure to be used by the kernel. Native functions should use the KernelFunction decorator in their definitions. They also use a Description field for parameters. For example: c#Copy [KernelFunction, Description("Convert an amount of currency to USD")] public static string ConvertCurrency( [Description("The currency")] string currency, [Description("The amount")] double amount) { // Code to convert currency } You can import your native functions into the kernel as a plugin. Classes containing native functions should be placed in a "Plugins" directory. Related functions should be placed in the same file to keep the code organized. You can also utilize subdirectories within the "Plugins" directory to further organize your code. For example, suppose you have a todo list application. A user wants to complete an item on their todo list. The large language model (LLM) can't access the user's todo list directly, but you can write a native function to access the list and mark an item as complete. For example, the todo list file might contain the following: OutputCopy { "todoList": [ { "task": "Complete coding exercise", "completed": false }, { "task": "Practice Mandarin", "completed": false }, { "task": "Buy groceries", "completed": false } ] } You can create a file TodoListPlugin.cs in the 'Plugins' directory with some code to mark the task completed: c#Copy using System.ComponentModel; using System.Text.Json; using System.Text.Json.Nodes; using Microsoft.SemanticKernel; public class TodoListPlugin { [KernelFunction, Description("Mark a todo list item as complete")] public static string CompleteTask([Description("The task to complete")] string task) { // Read the JSON file string jsonFilePath = $"{Directory.GetCurrentDirectory()}/todo.txt"; string jsonContent = File.ReadAllText(jsonFilePath); // Parse the JSON content JsonNode todoData = JsonNode.Parse(jsonContent); // Find the task and mark it as complete JsonArray todoList = (JsonArray) todoData["todoList"]; foreach (JsonNode taskNode in todoList) { if (taskNode["task"].ToString() == task) { taskNode["completed"] = true; break; } } // Save the modified JSON back to the file File.WriteAllText(jsonFilePath, JsonSerializer.Serialize(todoData)); return $"Task '{task}' marked as complete."; } } Notice the KernelFunction decorator on the CompleteTask function. This decorator tells the kernel that this function can be accessed. The Description decorator tells the kernel what the function does. The function accepts the task as a string. Variables of kernel functions should include a description that explains what the variable is. This function also returns a string letting the user know the task was marked complete. In the Program.cs file, you can import and call this native function similar to how you would call a one of the built-in plugins. For example: c#Copy using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Plugins.Core; var builder = Kernel.CreateBuilder(); builder.AddAzureOpenAIChatCompletion( "your-deployment-name", "your-endpoint", "your-api-key", "deployment-model"); var kernel = builder.Build(); kernel.ImportPluginFromType<TodoListPlugin>(); var result = await kernel.InvokeAsync<string>( "TodoListPlugin", "CompleteTask", new() {{ "task", "Buy groceries" }} ); Console.WriteLine(result); In this example, kernel.InvokeAsync is called with the plugin name, function name, and arguments. The task argument is set to "Buy groceries". The function will mark the task as complete in the todo list file and return a message to the user. Now your AI agent can help the user complete tasks on their todo list. Optionally, you can choose to denote the return type of the function in the InvokeAsync call. Otherwise a FunctionResult object is returned. In the next exercise, you'll practice creating your own plugins with native functions.

User
The Semantic Kernel SDK allows developers to run functions within prompts to create intelligent applications. Functions nested within your prompts can perform a wide range of tasks to make your AI agent more robust. This allows you to perform tasks that large language models can't typically complete on their own. For example, you can design a prompt with a function that can complete a designated item on a user's to-do list. Or you could design a prompt that uses a function to retrieve the user's music preferences and provide personalized concert recommendations. Nesting functions within prompts allows you to create complex interactions and provide more value to your users. In this module, you learn how to combine functions and prompts with the Semantic Kernel SDK. Learning objectives Practice creating plugins with the Semantic Kernel SDK. Learn how to combine prompts with native functions. Prerequisites Experience programming in C#. Visual Studio Code IDE installed. Familiarity with Azure and the Azure portal. Access to Azure OpenAI Services. Experience using the Semantic Kernel SDK to create prompt